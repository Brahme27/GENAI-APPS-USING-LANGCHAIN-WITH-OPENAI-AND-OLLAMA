{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f47902",
   "metadata": {},
   "source": [
    "## loading secret API keys from your environment so your program can use them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56629ab",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1bef11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'os' module to interact with the operating system\n",
    "import os \n",
    "\n",
    "# Import the 'load_dotenv' function from the 'dotenv' library\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file into the program\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the LangChain API key from the environment variable\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Enable LangChain Tracing v2 for tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Set the project name for LangChain from the environment variable\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a8e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000014F35FD5BD0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000014F35FF4970> root_client=<openai.OpenAI object at 0x0000014F3352B5E0> root_async_client=<openai.AsyncOpenAI object at 0x0000014F35FD5C90> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21065aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9bddfca6e2', 'id': 'chatcmpl-BYPWEf0Yh1kp6m0jQ9GVAQH9b9LvE', 'finish_reason': 'stop', 'logprobs': None} id='run-3a086111-2bc3-4dc7-8b5c-0cddc951e48d-0' usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d57fa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20c7f2",
   "metadata": {},
   "source": [
    "# Chat Prompt Template in Langchain\n",
    "## Definition\n",
    "A Chat Prompt Template is a ready-made message format with blanks (called placeholders) that you can fill with different information before sending it to an AI chatbot.\n",
    "\n",
    "## Uses\n",
    "Reuse messages with different inputs without rewriting the whole message.\n",
    "\n",
    "Keep conversations consistent and organized.\n",
    "\n",
    "Make your code cleaner and easier to maintain.\n",
    "\n",
    "Fill in user data dynamically (like name, question, or topic) to create personalized AI messages.\n",
    "\n",
    "## Example\n",
    "Template:\n",
    "\"Hello, my name is {name}. Can you help me with {topic}?\"\n",
    "\n",
    "## Fill with data:\n",
    "name = \"Alice\"\n",
    "topic = \"Python programming\"\n",
    "\n",
    "## Final message:\n",
    "\"Hello, my name is Alice. Can you help me with Python programming?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d101437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are an expert Ai Engineer.Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are an expert Ai Engineer.Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acb0e6",
   "metadata": {},
   "source": [
    "# Using this prompt template along with the LLM model by creating chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39c8c4",
   "metadata": {},
   "source": [
    "# Chain in Langchain\n",
    "## Definition\n",
    "A Chain connects multiple steps in order. The output of one step goes into the next step automatically.\n",
    "\n",
    "## Uses\n",
    "To do many things one after another.\n",
    "\n",
    "To automate tasks with the AI.\n",
    "\n",
    "To keep your code clean and organized.\n",
    "\n",
    "## Example\n",
    "If you want to:\n",
    "\n",
    "Ask AI to summarize something.\n",
    "\n",
    "Then translate that summary.\n",
    "\n",
    "A Chain helps you do both steps easily, one after the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944231af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of my last update in October 2023, the Prime Minister of India is Narendra Modi. However, please verify with a current source as this information may have changed after my last update.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 31, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BYPykS4s1SsYpFXuIbr8InANbUktm', 'finish_reason': 'stop', 'logprobs': None} id='run-185c7ff9-6f5d-47c8-a0fe-6448bb6d03df-0' usage_metadata={'input_tokens': 31, 'output_tokens': 39, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt | llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"who is PM of India?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cfbfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my last update in October 2023, the Prime Minister of India is Narendra Modi. However, please verify with a current source as this information may have changed after my last update.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b4f655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf04ef",
   "metadata": {},
   "source": [
    "# Output Parsers\n",
    "## Definition\n",
    "An Output Parser takes the raw text response from an AI model and cleans it up or changes its format so your program can use it easily.\n",
    "\n",
    "## Why use Output Parsers?\n",
    "AI sometimes gives extra text or formatting you don’t want.\n",
    "\n",
    "Parsers help you get only the useful part, like JSON, numbers, or simple strings.\n",
    "\n",
    "Makes it easier to work with AI answers in your code.\n",
    "\n",
    "#StrOutputParser\n",
    "## What is it?\n",
    "The StrOutputParser simply returns the AI response as a clean string without any extra processing.\n",
    "\n",
    "## When to use StrOutputParser?\n",
    "When you just want the raw text reply from the AI.\n",
    "\n",
    "When you don’t need to extract or change anything from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b7f99fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of October 2023, the Prime Minister of India is Narendra Modi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt | llm | output_parser\n",
    "response=chain.invoke({\"input\":\"who is PM of India?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0085ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you can observe that ,there are no unnessasary keys in the response,you got only the content(raw text)\n",
    "# that is the power of output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7723fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
