{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56a05d1",
   "metadata": {},
   "source": [
    "# Simple Generative Ai Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec05256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'os' module to interact with the operating system\n",
    "import os \n",
    "\n",
    "# Import the 'load_dotenv' function from the 'dotenv' library\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file into the program\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key from the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the LangChain API key from the environment variable\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Enable LangChain Tracing v2 for tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# Set the project name for LangChain from the environment variable\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "# This is a polite and sometimes required way to tell the server who's making the request (like a browser or a script).\n",
    "os.environ[\"USER_AGENT\"] = os.getenv(\"USER_AGENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7ed84",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "from websites we need to scrap the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc71b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38938ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x25f5e971780>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/administration/tutorials/manage_spend\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd175600",
   "metadata": {},
   "source": [
    "# Loads the entire website content into a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d5a0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()\n",
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25633cdf",
   "metadata": {},
   "source": [
    "## Load Data--->Docs---->divide our text into smaller chunks--->text---->vectors---->vector embeddings----->vector store Db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ae3f2",
   "metadata": {},
   "source": [
    "# Splits the document into smaller chunks of texts,because each LLM has different context window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952b5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "textsplitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents=textsplitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea86efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89e6e1",
   "metadata": {},
   "source": [
    "# Creating embedding for converting text to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ff382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e547ab5",
   "metadata": {},
   "source": [
    "# Creating a vector store DB for storing the vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0fa21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstoredb=FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "142fd417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x25f5240c910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2c02b",
   "metadata": {},
   "source": [
    "# Querying from vector storestore db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1b909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"We understand what usage looks like in terms of traces\"\n",
    "\n",
    "response=vectorstoredb.similarity_search(query, k=3)\n",
    "\n",
    "#response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d43c185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f82df0",
   "metadata": {},
   "source": [
    "# üîÅ Chain ‚Äì Simple Definition\n",
    "A Chain is a set of connected steps that process input and give output using a language model.\n",
    "\n",
    "üü¢ It connects things like prompts, models, and output formatting in one flow.\n",
    "\n",
    "# üìö Retriever ‚Äì Simple Definition\n",
    "A Retriever is a tool that finds and returns the most relevant information from a large set of documents.\n",
    "\n",
    "üü¢ It helps the AI answer questions using real data or files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e43892",
   "metadata": {},
   "source": [
    "# Creating LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fccbcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71594f00",
   "metadata": {},
   "source": [
    "# Retrieval chain and Document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "123aab86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000025F5EA2BDF0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000025F006F8B80>, root_client=<openai.OpenAI object at 0x0000025F006C58D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000025F5EA2BE20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "document_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e901b5",
   "metadata": {},
   "source": [
    "# Trying the document chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5586690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith has two usage limits: total traces and extended traces.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"LangSmith has two usage limits: total traces and extended\",\n",
    "    \"context\": [\n",
    "        Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph.\")\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb8fc2",
   "metadata": {},
   "source": [
    "# Add retrieval capability\n",
    "Turns your vectorstoredb (a searchable document database) into a retriever that can find documents for any question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c2dede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80e73662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025F5240C910>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000025F5EA2BDF0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000025F006F8B80>, root_client=<openai.OpenAI object at 0x0000025F006C58D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000025F5EA2BE20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6a7efb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To calculate a good \"total traces\" limit for production usage, follow these steps:\\n\\n1. **Understand Current Load**: \\n   - Determine the current usage of your application.\\n   - Example: An application called between 1.2-1.5 times per second logs around 100,000-130,000 traces per day.\\n\\n2. **Estimate Growth**:\\n   - Estimate how much you expect your usage to grow.\\n   - Example: Expect to double the current load in the near future.\\n\\n3. **Calculate Monthly Limit**:\\n   - Use a back-of-the-envelope calculation for the limit.\\n   - Formula: \\n     \\\\[\\n     \\\\text{limit} = \\\\text{current\\\\_load\\\\_per\\\\_day} \\\\times \\\\text{expected\\\\_growth} \\\\times \\\\text{days/month}\\n     \\\\]\\n   - Example Calculation:\\n     \\\\[\\n     \\\\text{limit} = 130,000 \\\\times 2 \\\\times 30 = 7,800,000 \\\\text{ traces / month}\\n     \\\\]\\n\\nSet this monthly limit for your usage in LangSmith to manage and optimize your costs efficiently.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a54688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(id='cf4c5c48-8641-4981-a89d-5bf92297d4e9', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Lets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:\\n\\nCurrent Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\\nmeaning we log around 100,000-130,000 traces per day\\nExpected Growth in Load: We expect to double in size in the near future.\\n\\nFrom these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:\\nlimit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month\\nWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:'),\n",
       "  Document(id='ab5ab1d5-c388-471d-acdf-29b3c0f1360f', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nnoteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\"),\n",
       "  Document(id='fc2c1e6d-d545-4ab9-8024-499f3ddf0853', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       "  Document(id='a6b0aed5-fb00-4021-b2a9-20080cd00c6f', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:')],\n",
       " 'answer': 'To calculate a good \"total traces\" limit for production usage, follow these steps:\\n\\n1. **Understand Current Load**: \\n   - Determine the current usage of your application.\\n   - Example: An application called between 1.2-1.5 times per second logs around 100,000-130,000 traces per day.\\n\\n2. **Estimate Growth**:\\n   - Estimate how much you expect your usage to grow.\\n   - Example: Expect to double the current load in the near future.\\n\\n3. **Calculate Monthly Limit**:\\n   - Use a back-of-the-envelope calculation for the limit.\\n   - Formula: \\n     \\\\[\\n     \\\\text{limit} = \\\\text{current\\\\_load\\\\_per\\\\_day} \\\\times \\\\text{expected\\\\_growth} \\\\times \\\\text{days/month}\\n     \\\\]\\n   - Example Calculation:\\n     \\\\[\\n     \\\\text{limit} = 130,000 \\\\times 2 \\\\times 30 = 7,800,000 \\\\text{ traces / month}\\n     \\\\]\\n\\nSet this monthly limit for your usage in LangSmith to manage and optimize your costs efficiently.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f0a7580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='cf4c5c48-8641-4981-a89d-5bf92297d4e9', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Lets start by setting limits on our production usage, since that is where the majority of spend comes from.\\nSetting a good total traces limit\\u200b\\nPicking the right \"total traces\" limit depends on the expected load of traces that you will send to LangSmith. You should\\nclearly think about your assumptions before setting a limit.\\nFor example:\\n\\nCurrent Load: Our gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it,\\nmeaning we log around 100,000-130,000 traces per day\\nExpected Growth in Load: We expect to double in size in the near future.\\n\\nFrom these assumptions, we can do a quick back-of-the-envelope calculation to get a good limit of:\\nlimit = current_load_per_day * expected_growth * days/month      = 130,000 * 2 * 30      = 7,800,000 traces / month\\nWe click on the edit icon on the right side of the table for our Prod row, and can enter this limit as follows:'),\n",
       " Document(id='ab5ab1d5-c388-471d-acdf-29b3c0f1360f', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content=\"The first metric tracks all traces that you send to LangSmith. The second tracks all traces that also have our Extended 400 Day Data Retention.\\nFor more details, see our data retention conceptual docs. Notice that these graphs look\\nidentical, which will come into play later in the tutorial.\\nLangSmith Traces usage is measured per workspace, because workspaces often represent development environments (as in our example),\\nor teams within an organization. As a LangSmith administrator, we want to understand spend granularly per each of these units. In\\nthis case where we just want to cut spend, we can focus on the environment responsible for the majority of costs first for the greatest savings.\\nnoteLangSmith's Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.\\nIn the above image, the vast majority of usage is in the workspace with ID c27dd32c-7c80-4e8c-acde-bfcb67a90ab2. We can\"),\n",
       " Document(id='fc2c1e6d-d545-4ab9-8024-499f3ddf0853', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Understand your current usage\\u200b\\nThe first step of any optimization process is to understand current usage. LangSmith gives two ways to do this: Usage Graph\\nand Invoices.\\nUsage Graph\\u200b\\nThe usage graph lets us examine how much of each usage based pricing metric we have consumed lately. It does not directly show\\nspend (which we will see later on our draft invoice).\\nWe can navigate to the Usage Graph under Settings -> Usage and Billing -> Usage Graph.\\n\\nWe see in the graph above that there are two usage metrics that LangSmith charges for:\\n\\nLangSmith Traces (Base Charge)\\nLangSmith Traces (Extended Data Retention Upgrades).'),\n",
       " Document(id='a6b0aed5-fb00-4021-b2a9-20080cd00c6f', metadata={'source': 'https://docs.smith.langchain.com/administration/tutorials/manage_spend', 'title': 'Optimize tracing spend on LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may\\nwant to be more liberal with your usage limits to avoid test failures.\\nNow that our limits are set, we can see that LangSmith shows a maximum spend estimate across all workspaces:')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de003764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
